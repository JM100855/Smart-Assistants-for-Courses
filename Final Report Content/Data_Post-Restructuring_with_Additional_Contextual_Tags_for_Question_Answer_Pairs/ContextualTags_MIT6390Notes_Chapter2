[
    {
        "type": "text",
        "text": "CHAPTER 3 Gradient Descent ",
        "page_idx": 0,
        "tags": [
            "introduction"
        ]
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0,
        "tags": [
            "introduction"
        ]
    },
    {
        "type": "text",
        "text": "In the previous chapter, we showed how to describe an interesting objective function for machine learning, but we need a way to find the optimal $\\Theta^{*}=\\mathsf{a r g}\\,\\mathsf{m i n}_{\\Theta}$ J$\\left(\\Theta\\right)$ , particularly when the objective function is not amenable to analytical optimization. For example, this can be the case when J$\\left(\\Theta\\right)$ involves a more complex loss function, or more general forms of regularization. It can also be the case when there is simply too much data for it to be computationally feasible to analytically invert the required matrices. ",
        "page_idx": 0,
        "tags": [
            "introduction"
        ]
    },
    {
        "type": "text",
        "text": "There is an enormous and fascinating literature on the mathematical and algorithmic foundations of optimization, but for this class, we will consider one of the simplest methods, called gradient descent. ",
        "page_idx": 0,
        "tags": [
            "introduction"
        ]
    },
    {
        "type": "text",
        "text": "Intuitively, in one or two dimensions, we can easily think of J$\\left(\\Theta\\right)$ as defining a surface over $\\Theta;$ ; that same idea extends to higher dimensions. Now, our objective is to find the $\\Theta$ value at the lowest point on that surface. One way to think about gradient descent is that you start at some arbitrary point on the surface, look to see in which direction the $\\prime\\mathrm{hill^{\\prime\\prime}}$ goes down most steeply, take a small step in that direction, determine the direction of steepest descent from where you are, take another small step, etc. ",
        "page_idx": 0,
        "tags": [
            "introduction"
        ]
    },
    {
        "type": "text",
        "text": "Below, we explicitly give gradient descent algorithms for one and multidimensional objective functions (Sections 3.1 and 3.2). We then illustrate the application of gradient descent to a loss function which is not merely mean squared loss (Section 3.3). And we present an important method known as stochastic gradient descent (Section 3.4), which is especially useful when datasets are too large for descent in a single batch, and has some important behaviors of its own. ",
        "page_idx": 0,
        "tags": [
            "introduction"
        ]
    },
    {
        "type": "text",
        "text": "Which you should consider studying some day! ",
        "page_idx": 0,
        "tags": [
            "introduction"
        ]
    },
    {
        "type": "text",
        "text": "We start by considering gradient descent in one dimension. Assume $\\Theta\\in\\mathbb{R},$ and that we know both $\\mathrm{J}(\\Theta)$ and its first derivative with respect to $\\Theta,$ ,$\\mathrm{J}^{\\prime}(\\Theta)$ . Here is pseudo-code for gradient descent on a arbitrary function f. Along with fand its gradient $\\nabla\\Theta^{\\mathsf{f}}$ (which, in the case of a scalar $\\Theta,$ , is the same as its derivative $\\mathsf{f}^{\\prime}$ ), we have to specify some hyperparameters. These hyper-parameters include the initial value for parameter $\\Theta,$ a step-size hyper-parameter \u03b7, and an accuracy hyper-parameter \u03f5.",
        "page_idx": 0,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "The hyper-parameter $\\boldsymbol\\upeta$ is often called learning rate when gradient descent is applied in machine learning. For simplicity, $\\boldsymbol\\upeta$ may be taken as a constant, as is the case in the pseudocode below; and we\u2019ll see adaptive (non-constant) step-sizes soon. What\u2019s important to notice though, is that even when $\\boldsymbol\\upeta$ is constant, the actual magnitude of the change to $\\Theta$ may not be constant, as that change depends on the magnitude of the gradient itself too. ",
        "page_idx": 0,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "1D-G RADIENT -D ESCENT $\\left(\\Theta_{i n i t},\\eta,\\mathsf{f},\\mathsf{f}^{\\prime},\\epsilon\\right)$   \n1 $\\Theta^{(0)}=\\Theta_{i n i t}$   \n2 t=0   \n3 repeat   \n4 5 $\\begin{array}{l}{\\mathrm{t}=\\mathrm{t}+\\mathrm{1}}\\\\ {\\Theta^{(\\mathrm{t})}=\\Theta^{(\\mathrm{t}-1)}-\\eta\\,\\mathrm{f}^{\\prime}\\big(\\Theta^{(\\mathrm{t}-1)}\\big)}\\end{array}$   \n6 until $|\\mathsf{f}(\\Theta^{(\\mathrm{t})})-\\mathsf{f}(\\Theta^{(\\mathrm{t}-1)})|<\\epsilon$   \n7 return $\\Theta^{(\\mathbf{t})}$ ",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "Note that this algorithm terminates when the change in the function fis sufficiently small. There are many other reasonable ways to decide to terminate, including: ",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "\u2022 Stop after a fixed number of iterations $\\mathsf{T}$ , i.e., when $\\mathrm{t}=\\mathsf{T}$ .  \n\u2022 Stop when the change in the value of the parameter $\\Theta$ is sufficiently small, i.e., when $\\left|\\Theta^{(\\mathbf{\\hat{t}})}-\\Theta^{(\\mathbf{t}-1)}\\right|<\\epsilon.$ .  \n\u2022 Stop when the derivative $\\mathsf{f}^{\\prime}$ at the latest value of $\\Theta$ is sufficiently small, i.e., when $\\left|\\mathsf{f}^{\\prime}\\big(\\Theta^{(\\mathrm{t})}\\big)\\right|<\\epsilon$ .",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "Study Question: Consider all of the potential stopping criteria for 1D-G RADIENT -D ESCENT , both in the algorithm as it appears and listed separately later. Can you think of ways that any two of the criteria relate to each other? ",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "Theorem 3.1.1. Choose any small distance $\\tilde{\\epsilon}\\,>\\,0$ . If we assume that fhas a minimum, is sufficiently \u201csmooth\u201d and convex, and if the step size \u03b7is sufficiently small, gradient descent will reach a point within \u02dcof a global optimum point $\\Theta$ .",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "However, we must be careful when choosing the step size to prevent slow convergence, non-converging oscillation around the minimum, or divergence. ",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "The following plot illustrates a convex function $\\mathsf{f}(\\mathsf{x})=(\\mathsf{x}{-}2)^{2}$ , starting gradient descent at $x_{\\mathrm{init}}=4.0$ with a step-size of $1/2$ . It is very well-behaved! ",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "A function is convex if the line segment between any two points on the graph of the function lies above or on the graph. ",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "image",
        "img_path": "images/266f4a8ba1a58fb12c2e32c1e1026b10be0bdc2c92b1dac3960b3117a83696fc.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "Study Question: What happens in this example with very small \u03b7? With very big \u03b7?",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "If fis non-convex, where gradient descent converges to depends on $x_{i n i t}$ . First, let\u2019s establish some definitions. Suppose we have analytically defined derivatives for f. Then we say that fhas a local minimum point or local optimum point at $x$ if $\\mathsf{f}^{\\prime}(\\mathsf{x})=0$ and $\\mathsf{f}^{\\prime\\prime}(\\mathsf{x})>0,$ ,and we say that $\\mathsf{f}(\\mathsf{x})$ is a local minimum value of f. More generally, $x$ is a local minimum point of $\\mathsf{f}$ if $\\mathsf{f}(\\mathsf{x})$ is at least as low as $\\mathsf{f}(\\ensuremath{\\boldsymbol{{x}}}^{\\prime})$ for all points $x^{\\prime}$ in some small area around $x$ . We say that fhas a global minimum point at $x$ if $\\mathsf{f}(\\mathsf{x})$ is at least as low as $\\mathsf{f}(\\ensuremath{\\boldsymbol{{x}}}^{\\prime})$ for every other input $x^{\\prime}$ . And then we call $\\mathsf{f}(\\mathsf{x})$ a global minimum value . A global minimum point is also a local minimum point, but a local minimum point does not have to be a global minimum point. ",
        "page_idx": 1,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "If fis non-convex (and sufficiently smooth), one expects that gradient descent (run long enough with small enough step size) will get very close to a point at which the gradient is zero, though we cannot guarantee that it will converge to a global minimum point. ",
        "page_idx": 2,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "There are two notable exceptions to this common sense expectation: First, gradient descent can get stagnated while approaching a point $x$ which is not a local minimum or maximum, but satisfies $\\mathsf{f}^{\\prime}(\\mathsf{x})\\,=\\,0$ . For example, for $\\mathsf{f}(\\mathsf{x})\\,=\\,\\mathsf{x}^{3}.$ , starting gradient descent from the initial guess $x_{i n i t}=1_{.}$ , while using step size $\\eta<1/3$ will lead to $\\bar{\\mathbf{x}^{(\\bar{\\mathbf{k}})}}$ converging to zero $\\mathtt{k}\\to\\infty$ ond, there are functi (even convex ones) with no minimum p ,like $\\mathsf{f}(\\mathsf{x})=\\exp(-\\mathsf{x}),$ , for which gradient descent with a positive step size converges to $+\\infty$ .",
        "page_idx": 2,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "The plot below shows two different $x_{i n i t},$ , and how gradient descent started from each \u221epoint heads toward two different local optimum points. ",
        "page_idx": 2,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "image",
        "img_path": "images/a83d643e0f26e2e0b1e8b5380c656b770419119b0d640b3c33b5433f5235c5da.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 2,
        "tags": [
            "gradient_descent_in_one_dimension"
        ]
    },
    {
        "type": "text",
        "text": "Th to the case of multi-dimensional $\\Theta$ s straightforward. Let\u2019s assume $\\Theta\\in\\mathbb{R}^{\\mathrm{m}}$ ,so f${\\boldsymbol{\\mathsf{f}}}:{\\mathbb{R}}^{m}\\rightarrow{\\mathbb{R}}.$ \u2192. The gradient of fwith respect to \u0398is ",
        "page_idx": 2,
        "tags": [
            "multiple_dimensions"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\nabla_{\\Theta}{\\mathsf{f}}={\\left[\\begin{array}{l}{\\mathsf{a f}/\\partial\\Theta_{1}}\\\\ {\\qquad\\vdots}\\\\ {\\partial{\\mathsf{f}}/\\partial\\Theta_{\\mathrm{m}}}\\end{array}\\right]}\n$$",
        "text_format": "latex",
        "page_idx": 2,
        "tags": [
            "multiple_dimensions"
        ]
    },
    {
        "type": "text",
        "text": "The algorithm remains the same, except that the update step in line 5 becomes ",
        "page_idx": 2,
        "tags": [
            "multiple_dimensions"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\Theta^{(\\mathrm{t})}=\\Theta^{(\\mathrm{t}-1)}-\\eta\\nabla_{\\Theta}\\mathsf{f}(\\Theta^{(\\mathrm{t}-1)})\n$$",
        "text_format": "latex",
        "page_idx": 2,
        "tags": [
            "multiple_dimensions"
        ]
    },
    {
        "type": "text",
        "text": "and any termination criteria that depended on the dimensionality of change. The easiest thing is to keep the test in line 6 as $\\left|\\mathsf{f}(\\Theta^{(\\mathrm{t})})-\\mathsf{\\bar{f}}(\\Theta^{(\\mathrm{t}-1)})\\right|\\,<\\,\\epsilon,$ $\\Theta$ would have to , which is sensible no matter the dimensionality of $\\Theta$ .",
        "page_idx": 2,
        "tags": [
            "multiple_dimensions"
        ]
    },
    {
        "type": "text",
        "text": "Study Question: Which termination criteria from the 1D case were defined in a way that assumes $\\Theta$ is one dimensional? ",
        "page_idx": 2,
        "tags": [
            "multiple_dimensions"
        ]
    },
    {
        "type": "text",
        "text": "Recall from the previous chapter that choosing a loss function is the first step in formulating a machine-learning problem as an optimization problem, and for regression we studied the mean square loss, which captures loss as (guess \u2212actual )2 . This leads to the ordinary least squares objective ",
        "page_idx": 3,
        "tags": [
            "application_to_regression"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\mathrm{J}(\\theta)=\\frac{1}{\\mathrm{n}}\\sum_{\\mathrm{i=1}}^{\\mathrm{n}}\\left(\\Theta^{\\mathsf{T}}x^{(\\mathrm{i})}-\\mathsf{y}^{(\\mathrm{i})}\\right)^{2}\\ \\,\n$$",
        "text_format": "latex",
        "page_idx": 3,
        "tags": [
            "application_to_regression"
        ]
    },
    {
        "type": "text",
        "text": "We use the gradient of the objective with respect to the parameters, ",
        "page_idx": 3,
        "tags": [
            "application_to_regression"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\nabla_{\\theta}\\mathsf{J}=\\frac{2}{\\mathsf{n}}\\underbrace{\\tilde{\\mathsf{X}}^{\\mathsf{T}}}_{\\mathsf{d}\\times\\mathsf{n}}\\underbrace{(\\tilde{\\mathsf{X}}\\theta-\\tilde{\\mathsf{Y}})}_{\\mathsf{n}\\times1}\\ ,\n$$",
        "text_format": "latex",
        "page_idx": 3,
        "tags": [
            "application_to_regression"
        ]
    },
    {
        "type": "text",
        "text": "to obtain an analytical solution to the linear regression problem. Gradient descent could also be applied to numerically compute a solution, using the update rule ",
        "page_idx": 3,
        "tags": [
            "application_to_regression"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\boldsymbol{\\theta}^{(\\mathrm{t})}=\\boldsymbol{\\theta}^{(\\mathrm{t-1})}-\\boldsymbol{\\eta}\\frac{2}{\\ n}\\sum_{\\mathrm{i}=1}^{\\ n}\\left(\\left[\\boldsymbol{\\theta}^{(\\mathrm{t}-1)}\\right]^{\\mathsf{T}}\\boldsymbol{x}^{(\\mathrm{i})}-\\boldsymbol{\\mathrm{y}}^{(\\mathrm{i})}\\right)\\boldsymbol{x}^{(\\mathrm{i})}\\,.\n$$",
        "text_format": "latex",
        "page_idx": 3,
        "tags": [
            "application_to_regression"
        ]
    },
    {
        "type": "text",
        "text": "Beware double superscripts! [\u03b8]Tis the transpose of the vector \u03b8",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Now, let\u2019s add in the regularization term, to get the ridge-regression objective: ",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\mathrm{J_{ridge}}(\\theta,\\theta_{0})=\\frac{1}{\\mathfrak{n}}\\sum_{\\mathrm{i}=1}^{\\mathfrak{n}}\\left(\\Theta^{\\mathsf{T}}x^{(\\mathrm{i})}+\\Theta_{0}-\\mathfrak{y}^{(\\mathrm{i})}\\right)^{2}+\\lambda\\|\\theta\\|^{2}\\;\\;.\n$$",
        "text_format": "latex",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Recall that in ordinary least squares, we finessed handling $\\uptheta_{0}$ by adding an extra dimension of all $1^{\\prime}\\mathrm{s}$ . In ridge regression, we really do need to separate the parameter vector \u03b8from the offset $\\theta_{0},$ and so, from the perspective of our general-purpose gradient descent method, our whole parameter set $\\Theta$ is defined to be $\\Theta=\\left(\\theta,\\theta_{0}\\right)$ . We will go ahead and find the gradients separately for each one: ",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla_{\\Theta}\\!\\int_{\\mathrm{ridge}}(\\Theta,\\Theta_{0})=\\frac{2}{\\mathsf{n}}\\sum_{\\mathrm{i}=1}^{\\mathsf{n}}\\left(\\Theta^{\\mathsf{T}}x^{(\\mathrm{i})}+\\Theta_{0}-\\mathbb{y}^{(\\mathrm{i})}\\right)x^{(\\mathrm{i})}+2\\lambda\\theta}}\\\\ {{\\displaystyle\\frac{\\partial\\mathrm{J_{ridge}}(\\Theta,\\Theta_{0})}{\\partial\\theta_{0}}=\\frac{2}{\\mathsf{n}}\\sum_{\\mathrm{i}=1}^{\\mathsf{n}}\\left(\\Theta^{\\mathsf{T}}x^{(\\mathrm{i})}+\\Theta_{0}-\\mathbb{y}^{(\\mathrm{i})}\\right)\\ \\,}}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Some passing familiarity with matrix derivatives is helpful here. A foolproof way of computing them is to compute partial derivative of Jwith respect to each component $\\bar{\\theta}_{\\mathrm{i}}$ of \u03b8. See Appendix A on matrix derivatives! ",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Note that $\\nabla_{\\boldsymbol{\\theta}}\\,{\\mathrm{J}}_{\\mathrm{ridge}}$ will be of shape $\\mathrm{~d~}\\times\\mathrm{~1~}$ and $\\partial]_{\\mathrm{ridge}}/\\partial\\theta_{0}$ will be a scalar since we have separated $\\uptheta_{0}$ from \u03b8here. ",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Study Question: Convince yourself that the dimensions of all these quantities are cor ct, under the assumption that \u03b8is $\\mathrm{~d~}\\times\\mathrm{~1~}$ . How does drelate to mas discussed for \u0398in the previous section? ",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "$\\nabla_{\\boldsymbol{\\theta}}\\left\\|\\boldsymbol{\\theta}\\right\\|^{2}$ by finding the partial derivatives $(\\partial\\left\\|\\dot{\\boldsymbol{\\theta}}\\right\\|^{2}/\\partial\\theta_{1},\\ldots,\\partial\\left\\|\\boldsymbol{\\theta}\\right\\|^{2}/\\partial\\theta_{\\mathrm{d}})$ \u2225\u2225\u2225\u2225. What is the shape of \u2207$\\nabla_{\\theta}\\parallel\\Theta\\|^{2}?$ \u2225\u2225",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Study Question: Compute $\\nabla_{\\boldsymbol{\\theta}}\\mathsf{J}_{\\mathrm{ridge}}(\\boldsymbol{\\theta}^{\\mathsf{T}}\\boldsymbol{x}\\!+\\!\\theta_{0},\\boldsymbol{\\ y})$ by finding the vector of partial derivatives $(\\partial)_{\\mathrm{ridge}}(\\theta^{\\mathsf{T}}x+\\theta_{0},\\mathfrak{y})/\\partial\\theta_{1},\\ldots,\\partial\\jmath_{\\mathrm{ridge}}(\\theta^{\\mathsf{T}}x+\\theta_{0},\\mathfrak{y})/\\partial\\theta_{\\mathrm{d}})$ .",
        "page_idx": 3,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Study Question: Use these last two results to verify our derivation above. ",
        "page_idx": 4,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Putting everything together, our gradient descent algorithm for ridge regression becomes ",
        "page_idx": 4,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "RR-G RADIENT -D ESCENT $\\left(\\theta_{i n i t},\\theta_{0i n i t},\\upeta,\\upepsilon\\right)$ ",
        "page_idx": 4,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{l}{\\theta^{(0)}=\\theta_{i n i t}}\\\\ {\\theta_{0}^{(0)}=\\theta_{0i n i t}}\\\\ {\\mathrm{~t=0~}}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 4,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{\\eta}^{\\star}\\;\\mathbf{\\dot{\\omega}}+\\mathbf{t}+\\mathbf{1}}\\\\ &{\\quad\\mathbf{\\Theta}\\mathbf{\\Theta}^{(\\mathbf{t})}=\\mathbf{\\Theta}^{(\\mathbf{t}-1)}-\\eta\\left(\\frac{1}{n}\\sum_{\\mathbf{i}=1}^{\\mathbf{n}}\\left(\\mathbf{\\Theta}^{(\\mathbf{t}-1)}\\mathbf{\\bar{x}}^{(\\mathbf{i})}+\\mathbf{\\Theta}_{0}^{(\\mathbf{t}-1)}-\\mathbf{y}^{(\\mathbf{i})}\\right)\\mathbf{x}^{(\\mathbf{i})}+\\lambda\\mathbf{\\Theta}^{(\\mathbf{t}-1)}\\right)}\\\\ &{\\quad\\mathbf{\\Theta}_{0}^{(\\mathbf{t})}=\\mathbf{\\Theta}_{0}^{(\\mathbf{t}-1)}-\\eta\\left(\\frac{1}{n}\\sum_{\\mathbf{i}=1}^{\\mathbf{n}}\\left(\\mathbf{\\Theta}^{(\\mathbf{t}-1)}\\mathbf{\\bar{Y}}_{\\mathbf{x}}^{(\\mathbf{i})}+\\mathbf{\\Theta}_{0}^{(\\mathbf{t}-1)}-\\mathbf{y}^{(\\mathbf{i})}\\right)\\right)}\\\\ &{\\quad\\mathbf{until}\\;\\left|\\int_{\\mathrm{ridge}}(\\mathbf{\\Theta}^{(\\mathbf{t})},\\mathbf{\\Theta}_{0}^{(\\mathbf{t})})-\\int_{\\mathrm{ridge}}(\\mathbf{\\Theta}^{(\\mathbf{t}-1)},\\mathbf{\\Theta}_{0}^{(\\mathbf{t}-1)})\\right|<\\epsilon}\\\\ &{\\quad\\mathbf{\\mathrm{return}}\\;\\mathbf{\\Theta}^{(\\mathbf{t})},\\mathbf{\\Theta}_{0}^{(\\mathbf{t})}}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 4,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Study Question: Is it okay that \u03bbdoesn\u2019t appear in line 7? ",
        "page_idx": 4,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "Study Question: Is it okay that the 2\u2019s from the gradient definitions don\u2019t appear in the algorithm? ",
        "page_idx": 4,
        "tags": [
            "ridge_regression"
        ]
    },
    {
        "type": "text",
        "text": "When the form of the gradient is a sum, rather than take one big(ish) step in the direction of the gradient, we can, instead, randomly select one term of the sum, and take a very small step in that direction. This seems sort of crazy, but remember that all the little steps would average out to the same direction as the big step if you were to stay in one place. Of course, you\u2019re not staying in that place, so you move, in expectation, in the direction of the gradient. ",
        "page_idx": 4,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "Most objective functions in machine learning can end up being written as a sum over data points, in which case, stochastic gradient descent ( SGD ) is implemented by picking a data point randomly out of the data set, computing the gradient as if there were only that one point in the data set, and taking a small step in the negative direction. ",
        "page_idx": 4,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "Let\u2019s assume our objective has the form The word \u201cstochastic\u201d means probabilistic, or random; so does \u201caleatoric,\u201d which is a very cool word. Look up aleatoric music, sometime. ",
        "page_idx": 4,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\mathsf{f}(\\Theta)=\\sum_{\\mathrm{i}=1}^{\\mathrm{n}}\\mathsf{f}_{\\mathrm{i}}(\\Theta)\\,\\mathrm{~,~}\n$$",
        "text_format": "latex",
        "page_idx": 4,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "where nis the number of data points used in the objective (and this may be different from the number of points available in the whole data set). Here is pseudocode for applying SGD to such an objective f; it assumes we know the form of $\\nabla\\Theta^{\\boldsymbol{\\mathsf{f}}_{\\mathrm{i}}}$ for all i in 1 . . . n:",
        "page_idx": 4,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "STOCHASTIC -G RADIENT -D ESCENT (\u0398init ,\u03b7,f,\u2207\u0398f1 , . . . , \u2207\u0398fn,T)",
        "page_idx": 4,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "1 $\\Theta^{(0)}=\\Theta_{i n i t}$   \n2 for $\\mathrm{~t~}=1$ to T  \n3 randomly select $\\mathfrak{i}\\in\\{1,2,\\dots,\\mathfrak{n}\\}$   \n4 $\\Theta^{(\\mathrm{t})}=\\bar{\\Theta^{(\\mathrm{t}-1)}}-\\boldsymbol{\\upeta}(\\mathrm{t})\\,\\nabla_{\\Theta}\\mathrm{f}_{\\mathrm{i}}\\big(\\Theta^{(\\mathrm{t}-1)}\\big)$   \n5 return $\\Theta^{(\\mathbf{t})}$ ",
        "page_idx": 4,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "Note that now instead of a fixed value of $\\boldsymbol{\\mathsf{n}},\\boldsymbol{\\mathsf{n}}$ is indexed by the iteration of the algorithm, t. Choosing a good stopping criterion can be a little trickier for SGD than traditional gradient descent. Here we\u2019ve just chosen to stop after a fixed number of iterations T.",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "For SGD to converge to a local optimum point as tincreases, the step size has to decrease as a function of time. The next result shows one step size sequence that works. ",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "Theorem 3.4.1. If fis convex, and $\\upeta(\\up t)$ is a sequence satisfying ",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "equation",
        "text": "$$\n\\sum_{\\mathrm{t=1}}^{\\infty}\\boldsymbol{\\upeta}(\\mathrm{t})=\\infty\\;\\,a n d\\ \\sum_{\\mathrm{t=1}}^{\\infty}\\boldsymbol{\\upeta}(\\mathrm{t})^{2}<\\infty\\ \\,\n$$",
        "text_format": "latex",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "then SGD converges with probability one to the optimal \u0398.",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "Why these two ons? The intuition is that the first condition, on $\\sum\\mathsf{\\boldsymbol{\\mathsf{n}}(t)}.$ , is needed to allow for the possibility of an unbounded potential range of exploration, while the second condition, on $\\sum\\mathsf{\\boldsymbol{\\upeta}}(\\mathsf{t})^{2}$ , ensures that the step sizes and smaller as tincreases. ",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "One \u201clegal\u201d way of setting the step size is to make P$\\upeta(\\mathrm{t})=1/\\mathrm{t}$ but people often use rules that decrease more slowly, and so don\u2019t strictly satisfy the criteria for convergence. ",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "Study Question: If you start a long way from the optimum, would making \u03b7(t)decrease more slowly tend to make you move more quickly or more slowly to the optimum? ",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "There are multiple intuitions for why SGD might be a better choice algorithmically than regular GD (which is sometimes called batch GD (BGD )): ",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "\u2022BGD typically requires computing some quantity over every data point in a data set. SGD may perform well after visiting only some of the data. This behavior can be useful for very large data sets \u2013 in runtime and memory savings. \u2022 If your fis actually non-convex, but has many shallow local optimum points that might trap BGD , then taking samples from the gradient at some point $\\Theta$ might \u201cbounce\u201d you around the landscape and away from the local optimum points. \u2022 Sometimes, optimizing freally well is not what we want to do, because it might overfit the training set; so, in fact, although SGD might not get lower training error than BGD , it might result in lower test error. ",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    },
    {
        "type": "text",
        "text": "We have left out some gnarly conditions in this theorem. Also, you can learn more about the subtle difference between \u201cwith probability one\u201d and \u201calways\u201d by taking an advanced probability course. ",
        "page_idx": 5,
        "tags": [
            "stochastic_gradient_descent"
        ]
    }
]